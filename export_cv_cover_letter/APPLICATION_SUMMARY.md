# Job Application Summary - 16 October 2025

This directory contains tailored CV and cover letter packages for four data engineering contract positions in Australia.

## Applications Created

### 1. Long Resources Ltd - Data Engineer (Sydney)
**Directory:** `2025-10-16_long_resources_data_engineer_sydney/`
**Location:** Sydney, New South Wales
**Contract Type:** Contract
**Key Skills Emphasised:** Python, PySpark, SQL, Azure, AWS, Synapse, Databricks, ETL, Data Engineering, Data Pipeline

**Job Focus:**
- Scalable data pipelines and ETL processes
- Cloud-based data warehouse solutions
- Data quality frameworks
- BAU operations and incident management

**Documents:**
- Linus_McManamey_CV.docx
- Linus_McManamey_CL.docx
- job_info.json

---

### 2. NTT DATA - Data Engineer BAU Integration & Cloud Pipelines
**Directory:** `2025-10-16_ntt_data_cloud_pipelines/`
**Location:** New South Wales, Australia
**Contract Type:** Contract (Actively Hiring)
**Key Skills Emphasised:** Python, PySpark, SQL, Azure, Synapse, Databricks, ETL, Data Engineering, CI/CD

**Job Focus:**
- BAU support for data integration platforms
- Azure cloud services (Data Factory, Synapse Analytics, Azure Functions)
- Cloud-native data pipeline development
- Enterprise-scale systems

**Documents:**
- Linus_McManamey_CV.docx
- Linus_McManamey_CL.docx
- job_info.json

---

### 3. NTT DATA - Data Engineer BAU (MuleSoft Integration - Mandatory)
**Directory:** `2025-10-16_ntt_data_data_engineer_bau/`
**Location:** New South Wales, Australia
**Contract Type:** Contract (Actively Hiring)
**Key Skills Emphasised:** Python, SQL, Azure, AWS, ETL, Data Engineering, CI/CD, Agile, Java, API

**Job Focus:**
- MuleSoft Anypoint Platform integration (mandatory requirement)
- BAU support for integration platforms
- API design and development
- DataWeave transformations

**Note:** This role requires MuleSoft certifications and hands-on experience. If you don't have MuleSoft experience, you may want to skip this application or highlight transferable API integration skills.

**Documents:**
- Linus_McManamey_CV.docx
- Linus_McManamey_CL.docx
- job_info.json

---

### 4. Talent Insights Group - Principal Data Engineer
**Directory:** `2025-10-16_talent_insights_principal_data_engineer/`
**Location:** Remote (Australia-wide)
**Contract Type:** Contract (Actively Hiring)
**Key Skills Emphasised:** Python, PySpark, Azure, AWS, Synapse, Databricks, ETL, Data Engineering, Kubernetes, CI/CD

**Job Focus:**
- Architectural design of enterprise-scale data platforms
- Technical leadership and mentoring
- Modern data architectures (medallion, data mesh, lakehouse)
- Thought leadership and innovation
- 8+ years experience with 3+ years in senior/lead roles required

**Documents:**
- Linus_McManamey_CV.docx
- Linus_McManamey_CL.docx
- job_info.json

---

## Next Steps

1. **Review Each Application Package**
   - Open each directory and review the tailored CV and cover letter
   - Ensure the documents accurately reflect your experience
   - Customise further if needed based on specific job requirements

2. **Prioritise Applications**
   - **High Priority:** Long Resources (Sydney-based, general data engineering)
   - **High Priority:** NTT DATA Cloud Pipelines (Azure focus, matches your skills)
   - **Medium Priority:** Talent Insights Principal (remote, senior level, premium rate)
   - **Low Priority:** NTT DATA MuleSoft (only if you have MuleSoft experience)

3. **Additional Customisation Recommended**
   - Review each job posting on LinkedIn for any specific requirements
   - Add specific examples from your experience that match job criteria
   - Ensure cover letters address company-specific values and culture

4. **Application Submission**
   - Submit applications through LinkedIn or company career portals
   - Keep track of application dates and follow-up schedules
   - Prepare for potential phone screens highlighting relevant Azure/Synapse experience

## Key Strengths to Emphasise

Based on your background (inferred from the medallion architecture work):
- Azure Synapse Analytics expertise
- PySpark and Python data engineering
- ETL/ELT pipeline development
- Data warehouse and data lake implementations
- CI/CD and DevOps practices
- Data quality frameworks
- Production support and BAU operations

## Documents Location
All application packages are saved in:
`/workspaces/Dev/need_a_new_job/export_cv_cover_letter/`

Good luck with your applications!
