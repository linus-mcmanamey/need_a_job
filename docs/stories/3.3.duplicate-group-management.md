# Story 3.3: Duplicate Group Management (Tier 1 Fuzzy Matching)

## Status
ðŸŸ¡ Planning - Ready for Implementation

**Complexity:** Medium
**Estimated Hours:** 3.5 hours
**Story Points:** 5

---

## Story Overview

Implement Tier 1 duplicate detection using fuzzy matching algorithms to identify obvious job duplicates across platforms. This story creates the foundation for deduplication by automatically grouping jobs that are clearly the same posting (â‰¥90% similarity) and flagging borderline matches (75-89%) for deeper analysis in Story 3.4.

**As a** system
**I want to** quickly identify obvious job duplicates using fuzzy matching
**So that** I don't waste time on detailed analysis for every job pair

---

## Acceptance Criteria

1. **Fuzzy matching algorithms implemented (REQ-002, Section 4.4):**
   - Title: token_set_ratio (RapidFuzz)
   - Company: fuzzy_match (basic string similarity)
   - Description: fuzzy_match (first 500 characters)
   - Location: fuzzy_normalized (handle variations)

2. **Weighted scoring applied:**
   - Title: 20%
   - Company: 10%
   - Description: 50%
   - Location: 20%

3. **Similarity score calculated for each new job:**
   - Compare against jobs from last 30 days
   - Calculate combined similarity score (0.0 to 1.0)

4. **Thresholds applied (REQ-002):**
   - â‰¥ 90%: Auto-group as duplicate
   - 75-89%: Flag for Tier 2 deep analysis
   - < 75%: Different jobs

5. **Duplicate grouping:**
   - Create duplicate_group_id for first job in group
   - Assign same duplicate_group_id to duplicates

6. **Database updates:**
   - Set duplicate_group_id in jobs table
   - Create application_tracking with status="duplicate" for secondary jobs
   - Primary job (first discovered) remains active

7. **Logs duplicate detection decisions** for audit trail

---

## Implementation Tasks

### Task 1: Create Fuzzy Matching Service
Create `app/services/fuzzy_matcher.py` with core matching algorithms:
- Implement normalize_string() for consistent comparisons
- Implement location_normalize() for location variations
- Implement fuzzy_match() with RapidFuzz token_set_ratio
- Implement weighted_similarity_score() combining all metrics
- Error handling for invalid inputs
- **Estimated:** 1.0 hour

### Task 2: Create Duplicate Detection Service
Create `app/services/duplicate_detector.py` with business logic:
- Implement find_job_duplicates(job_id) -> List[Job]
- Query jobs from last 30 days with similar titles (pre-filter)
- Calculate similarity scores for each candidate
- Classify matches: duplicate (â‰¥90%), analyze (75-89%), different (<75%)
- Log all classification decisions
- **Estimated:** 1.0 hour

### Task 3: Implement Duplicate Grouping
Update `app/repositories/jobs_repository.py` and create grouping logic:
- Implement assign_duplicate_group() to set duplicate_group_id
- Create get_primary_job() in group
- Implement get_group_members() for all jobs in group
- Add query filters for duplicate groups
- **Estimated:** 0.5 hour

### Task 4: Create Application Tracking Integration
Update `app/repositories/application_repository.py`:
- Implement create_duplicate_tracking(job_id, primary_job_id)
- Set status="duplicate" for non-primary jobs
- Copy stage_outputs and completed_stages from primary
- Link all jobs in group via duplicate_group_id
- **Estimated:** 0.5 hour

### Task 5: Create Agent/Pipeline Integration
Create `app/agents/duplicate_detection_agent.py`:
- Trigger on new job discovery (from Story 3.1/3.2 pollers)
- Call duplicate_detector.find_job_duplicates()
- Update jobs and applications based on similarity scores
- Return structured results (duplicates found, threshold met, group_id)
- **Estimated:** 0.3 hour

### Task 6: Write Comprehensive Tests
Create test suite in `tests/unit/services/` and `tests/integration/`:
- Unit tests for fuzzy_matcher.py (normalize, scoring, algorithms)
- Unit tests for duplicate_detector.py (classification logic)
- Integration test: New job â†’ detection â†’ grouping â†’ application tracking
- Edge cases: Special characters, multi-word titles, abbreviations
- **Estimated:** 0.15 hour (pytest template-driven)

### Task 7: Create API Endpoints (Optional)
Add REST endpoints to FastAPI if needed for debugging:
- GET /api/duplicates/{job_id} - Get duplicates for a job
- GET /api/duplicate-groups - List all groups
- POST /api/duplicates/analyze - Manual duplicate check
- **Estimated:** 0.05 hour

---

## Test Strategy

### Unit Tests
- **Fuzzy Matching:** Test all normalization functions, matching algorithms, weighted scoring
- **Duplicate Detection:** Test classification logic, threshold application, pre-filtering
- **Test Data:** Various job formats, special characters, abbreviations, location variations

### Integration Tests
- **End-to-End:** New job discovery â†’ fuzzy matching â†’ grouping â†’ application tracking
- **Edge Cases:** Empty results, all jobs similar, no similar jobs
- **Performance:** 50+ jobs analyzed for duplicates (should complete in <2s)

### Manual Testing
- Verify logs capture all classification decisions
- Check duplicate_group_id populated correctly in database
- Validate application_tracking created with "duplicate" status

---

## Complexity Assessment

**Complexity: MEDIUM**

**Reasoning:**
- Fuzzy matching is well-established (using RapidFuzz library)
- Weighted scoring is straightforward math
- Database operations already exist (from Story 1.3)
- Main complexity: Balancing performance with accuracy
- Pre-filtering (similar titles) critical for performance

**Risk Areas:**
- Threshold tuning (90% vs 75% cutoffs) - may need adjustment after testing
- Performance with 1000+ jobs (requires pre-filtering)
- Location normalization edge cases

**Mitigations:**
- Use proven RapidFuzz library
- Pre-filter by title similarity before detailed comparison
- Comprehensive test suite with edge cases
- Logging for post-analysis if thresholds need adjustment

---

## Implementation Notes

### Database Schema (Already Exists - Story 1.3)
```sql
-- jobs table already has:
- duplicate_group_id (UUID, nullable)
- job_title, company_name, job_description, location
- discovered_timestamp (for 30-day filter)

-- application_tracking already has:
- status (can be "duplicate")
- stage_outputs (for copying from primary)
```

### Key Dependencies
- **RapidFuzz:** For token_set_ratio matching (`pip install rapidfuzz`)
- **Jobs Repository:** For database access
- **Application Repository:** For tracking updates
- **Loguru:** For audit logging

### Performance Optimization
- Pre-filter candidates: Only compare against jobs with similar titles (token_set_ratio > 0.5)
- Limit comparison window: Last 30 days only
- Consider async processing for large datasets (future: background task)

### Configuration
Add to `config.yml`:
```yaml
duplicate_detection:
  similarity_threshold_duplicate: 0.90
  similarity_threshold_analyze: 0.75
  days_lookback: 30
  weights:
    title: 0.20
    company: 0.10
    description: 0.50
    location: 0.20
```

---

## Definition of Done

- [ ] Fuzzy matcher service implemented and tested
- [ ] Duplicate detector service implemented and tested
- [ ] Duplicate grouping logic implemented
- [ ] Application tracking integration complete
- [ ] Agent integration complete
- [ ] API endpoints added (optional)
- [ ] All unit tests passing (100% pass rate)
- [ ] Integration tests passing
- [ ] Performance validated (<2s for 50 jobs)
- [ ] Logging captures all decisions
- [ ] Code reviewed and merged
- [ ] Story document marked complete

---

## Links & References

**PRD:** docs/prd/epic-3-duplicate-detection.md#story-33
**Database Schema:** docs/stories/1.3.duckdb-schema.md
**Epic:** Epic 3 - Duplicate Detection (Weeks 3-4)
**Related Stories:** 3.1 (SEEK Poller), 3.2 (Indeed Poller), 3.4 (LLM Embeddings)

---

## Change Log

| Date | Version | Description |
|------|---------|-------------|
| 2025-10-29 | 1.0 | Story 3.3 planning document created |
