# Story 3.1: SEEK Job Poller

## Status
In Progress

**Started:** 2025-10-29
**Branch:** feature/story-3-1-seek-job-poller

## Story
**As a** system,
**I want to** search SEEK for jobs matching my criteria,
**So that** I discover jobs from Australia's largest job platform.

## Context
First story in Epic 3: Duplicate Detection. Extends job discovery beyond LinkedIn to SEEK (Australia's largest job platform). This poller will use web scraping (BeautifulSoup/Playwright) to search and extract job data from SEEK.

## Dependencies
- âœ… Epic 1: Foundation (database, models, config)
- âœ… Epic 2: Core Agents (for processing discovered jobs)
- ðŸ“¦ BeautifulSoup4 or Playwright (web scraping)
- ðŸ“¦ requests library

## Acceptance Criteria

### AC 1: SEEKPoller Class Implementation
- [ ] Create `SEEKPoller` in `app/pollers/seek_poller.py`
- [ ] Similar structure to LinkedInPoller
- [ ] Initialize with config, repositories
- [ ] Base URL: https://www.seek.com.au
- [ ] Search endpoint: /data-engineer-jobs (configurable)

### AC 2: Web Scraping Implementation
- [ ] Use BeautifulSoup or Playwright for scraping
- [ ] Handle JavaScript-rendered content if needed
- [ ] Respect robots.txt
- [ ] Add random delays (2-5 seconds) between requests
- [ ] User agent spoofing to avoid anti-bot measures
- [ ] Pagination handling (multiple pages of results)

### AC 3: Job Metadata Extraction
- [ ] Extract company name, job title, location
- [ ] Extract salary (handle various formats: "$100k-$120k", ranges)
- [ ] Extract posting date
- [ ] Extract full job description
- [ ] Extract requirements and responsibilities
- [ ] Store platform_source='seek'
- [ ] Store job URL

### AC 4: Rate Limiting
- [ ] Rate limit: 50 requests/hour
- [ ] Use RateLimiter class (from LinkedIn poller)
- [ ] Add random delays between requests (2-5s)
- [ ] Handle rate limit exceeded (wait and resume)

### AC 5: Database Integration
- [ ] Insert new jobs into `jobs` table
- [ ] Check for duplicates (same job_url)
- [ ] Create application_tracking records with status="discovered"
- [ ] Use JobsRepository and ApplicationRepository

### AC 6: Error Handling
- [ ] Handle invalid HTML structure (log and skip job)
- [ ] Handle network timeouts (retry with backoff)
- [ ] Handle rate limit exceeded (wait)
- [ ] Handle missing required fields (log and skip)
- [ ] Always continue polling after errors

### AC 7: Poller Configuration
- [ ] Add seek section to config/search.yaml:
  ```yaml
  seek:
    enabled: true
    search_terms: ["data engineer", "data engineering"]
    location: "Australia"
    rate_limit_requests_per_minute: 1  # 50 per hour
    user_agent: "Mozilla/5.0 ..."
    delay_between_requests_seconds: [2, 5]  # random range
  ```

### AC 8: Metrics and Logging
- [ ] Track metrics:
  - jobs_found
  - jobs_inserted
  - duplicates_skipped
  - errors
- [ ] Log each discovered job
- [ ] Log errors and retries
- [ ] Log rate limiting events

### AC 9: Testing
- [ ] Unit tests for SEEKPoller
- [ ] Test metadata extraction
- [ ] Test salary parsing (SEEK-specific formats)
- [ ] Test duplicate detection
- [ ] Test error handling
- [ ] Mock HTTP responses for testing
- [ ] Minimum 80% coverage

### AC 10: Integration with Job Processing
- [ ] Discovered jobs flow into agent pipeline
- [ ] Status: "discovered" after insertion
- [ ] Ready for JobMatcher agent processing

## Implementation Plan

### Phase 1: Setup and Structure (30 min)
1. Create SEEKPoller class skeleton
2. Set up web scraping library (BeautifulSoup)
3. Add configuration to search.yaml
4. Basic structure tests

### Phase 2: Web Scraping (60 min)
1. Implement SEEK search URL construction
2. Implement HTML fetching with rate limiting
3. Implement pagination handling
4. Parse job listings from search results
5. Test with real SEEK pages (saved HTML)

### Phase 3: Metadata Extraction (45 min)
1. Implement extract_job_metadata()
2. Parse company name, title, location
3. Parse salary (SEEK-specific formats)
4. Parse posting date
5. Parse description, requirements
6. Tests for extraction

### Phase 4: Database Integration (30 min)
1. Integrate with JobsRepository
2. Implement duplicate detection (job_url)
3. Insert jobs into database
4. Create application_tracking records
5. Tests for database operations

### Phase 5: Error Handling & Testing (45 min)
1. Comprehensive error handling
2. Retry logic with backoff
3. Rate limiting enforcement
4. Unit tests (80%+ coverage)
5. Integration tests (optional)

**Total: 3.5 hours**

## Technical Details

### SEEK URL Structure

```python
# Search URL format
base_url = "https://www.seek.com.au"
search_url = f"{base_url}/data-engineer-jobs/in-All-Australia"

# With pagination
page_url = f"{search_url}?page={page_number}"
```

### HTML Parsing Example

```python
from bs4 import BeautifulSoup
import requests

def fetch_seek_jobs(search_term: str, location: str, page: int = 1):
    """Fetch jobs from SEEK search results."""
    url = f"https://www.seek.com.au/{search_term}-jobs/in-{location}?page={page}"

    headers = {
        "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36"
    }

    response = requests.get(url, headers=headers)
    soup = BeautifulSoup(response.content, 'html.parser')

    # Extract job listings
    job_cards = soup.find_all('article', {'data-automation': 'normalJob'})

    for card in job_cards:
        title = card.find('a', {'data-automation': 'jobTitle'}).text
        company = card.find('a', {'data-automation': 'jobCompany'}).text
        location = card.find('a', {'data-automation': 'jobLocation'}).text
        # ...extract more fields
```

### Salary Parsing (SEEK-specific)

```python
def _parse_seek_salary(salary_str: str | None) -> Decimal | None:
    """
    Parse SEEK salary formats:
    - "$100,000 - $120,000" (annual)
    - "$1000 per day"
    - "$1000-$1200 per day"
    - "Competitive salary" (None)
    """
    if not salary_str or "competitive" in salary_str.lower():
        return None

    # Extract numbers and determine if daily or annual
    # Convert annual to daily (/ 230 working days)
    # Return average if range
```

### Rate Limiting

```python
class SEEKPoller:
    def __init__(self, config, jobs_repo, app_repo):
        # ...
        rate_limit = config.get("seek", {}).get("rate_limit_requests_per_minute", 1) * 60
        self.rate_limiter = RateLimiter(calls_per_hour=rate_limit)

        delay_range = config.get("seek", {}).get("delay_between_requests_seconds", [2, 5])
        self.min_delay, self.max_delay = delay_range

    def fetch_page(self, url: str):
        """Fetch page with rate limiting and delays."""
        self.rate_limiter.wait_if_needed()

        # Random delay to appear human-like
        delay = random.uniform(self.min_delay, self.max_delay)
        time.sleep(delay)

        response = requests.get(url, headers=self.headers)
        return response
```

## Definition of Done

- [ ] SEEKPoller class implemented
- [ ] Web scraping working (BeautifulSoup or Playwright)
- [ ] Metadata extraction for all required fields
- [ ] Rate limiting enforced (50 req/hour)
- [ ] Database integration (jobs table, application_tracking)
- [ ] Error handling comprehensive
- [ ] Unit tests pass â‰¥80% coverage
- [ ] No regressions (all existing tests pass)
- [ ] Configuration added to search.yaml
- [ ] Code reviewed and approved
- [ ] Documentation complete

**Story Status:** In Progress
**Complexity:** MEDIUM (web scraping + integration)
**Expected Duration:** 3.5 hours

## Notes

### SEEK Anti-Bot Measures
- May require JavaScript rendering (use Playwright if BeautifulSoup fails)
- Rotate user agents periodically
- Use realistic request patterns (delays, pagination)
- Respect robots.txt

### Testing Strategy
- Save sample SEEK HTML pages for testing
- Mock HTTP requests in unit tests
- Test with various salary formats
- Test with missing data (no salary, etc.)

### Future Enhancements
- Support for advanced search filters (salary range, work type)
- Support for saved searches (SEEK accounts)
- Email notifications for new jobs
- Integration with SEEK's API (if available)
