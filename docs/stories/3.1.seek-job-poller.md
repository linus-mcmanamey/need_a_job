# Story 3.1: SEEK Job Poller

## Status
Ready for Implementation

**Created:** 2025-10-29
**Epic:** Epic 3 - Duplicate Detection
**Story Points:** Medium (3.5 hours)
**Branch:** feature/story-3-1-seek-job-poller

## Story Overview

**As a** system,
**I want to** search SEEK for jobs matching my criteria,
**So that** I discover jobs from Australia's largest job platform.

### Context
First story in Epic 3: Duplicate Detection. Extends job discovery beyond LinkedIn to SEEK (Australia's largest job platform). This poller will use web scraping (BeautifulSoup/Playwright) to search and extract job data from SEEK, following the same patterns established by the LinkedInPoller in Story 1.4.

SEEK is Australia's #1 employment marketplace and will significantly expand our job discovery coverage. Unlike LinkedIn which has MCP API access, SEEK requires web scraping which introduces additional complexity around:
- JavaScript rendering
- Anti-bot measures
- Rate limiting
- HTML parsing variability

## Dependencies

### Prerequisite Stories
- âœ… Story 1.1: Project Setup (FastAPI, DuckDB, dependencies)
- âœ… Story 1.2: Configuration System (YAML config loading)
- âœ… Story 1.3: DuckDB Schema (jobs table, application_tracking table)
- âœ… Story 1.4: LinkedIn Job Poller (establishes poller pattern)
- âœ… Story 1.5: Job Queue System (Redis + RQ workers)
- âœ… Epic 2: Core Agents (for processing discovered jobs)

### External Dependencies
- ðŸ“¦ BeautifulSoup4 (for HTML parsing) - needs installation
- ðŸ“¦ requests library (HTTP requests) - already installed
- ðŸ“¦ Playwright (if JavaScript rendering needed) - optional fallback
- ðŸ”§ Docker MCP Gateway (browser tools available)

### Configuration Dependencies
- config/search.yaml - existing, needs SEEK section
- config/platforms.yaml - existing, needs SEEK configuration

## Acceptance Criteria

### AC 1: SEEK Web Scraping Implementation (REQ-001)
**Copied from PRD:**
1. SEEK web scraper implemented:
   - Base URL: https://www.seek.com.au
   - Search: /data-engineer-jobs with location filters
   - Rate limiting: 50 requests/hour
2. Handles SEEK-specific challenges:
   - JavaScript-rendered content (use Playwright if needed)
   - Anti-bot measures (user agent, delays between requests)
   - Pagination (multi-page results)

**Implementation Details:**
- [ ] Create `SEEKPoller` class in `app/pollers/seek_poller.py`
- [ ] Similar structure to `LinkedInPoller` (Story 1.4)
- [ ] Initialize with config, jobs_repository, application_repository
- [ ] Use BeautifulSoup for HTML parsing (primary approach)
- [ ] Fallback to Playwright/Browser MCP if JavaScript rendering needed
- [ ] Respect robots.txt
- [ ] Add random delays (2-5 seconds) between requests
- [ ] User agent spoofing to avoid anti-bot measures
- [ ] Handle pagination across multiple pages

### AC 2: Job Metadata Extraction (REQ-003)
**Copied from PRD:**
2. Job metadata extraction:
   - Company name, job title, salary, location, posting date
   - Full job description, requirements, responsibilities
   - Platform source (seek) and job URL

**Implementation Details:**
- [ ] Extract company name from job card/page
- [ ] Extract job title from job card/page
- [ ] Extract location with SEEK-specific format handling
- [ ] Extract salary with format parsing (see AC 5)
- [ ] Extract posting date and convert to DATE format
- [ ] Extract full job description (may require clicking through)
- [ ] Extract requirements section (if available)
- [ ] Extract responsibilities section (if available)
- [ ] Store platform_source='seek'
- [ ] Store job_url for duplicate detection

### AC 3: SEEK-Specific Format Handling (REQ-003)
**Copied from PRD:**
5. Handles SEEK-specific formats:
   - Salary ranges ("$100k-$120k")
   - Location variations ("Melbourne CBD", "Remote - VIC")

**Implementation Details:**
- [ ] Parse salary formats:
  - Annual ranges: "$100,000 - $120,000 per annum"
  - Daily rates: "$1000 per day"
  - Daily ranges: "$1000-$1200 per day"
  - Hourly rates: "$50-$70 per hour"
  - "Competitive salary" â†’ None
- [ ] Convert all salaries to AUD per day (230 working days/year)
- [ ] Handle salary range by averaging min/max
- [ ] Normalize location formats:
  - "Melbourne CBD" âœ“
  - "Remote - VIC" âœ“
  - "Hobart, TAS" âœ“
  - "Hybrid - Melbourne" âœ“

### AC 4: Rate Limiting (REQ-001)
**Copied from PRD:**
1. Rate limiting: 50 requests/hour
7. Error handling:
   - Rate limit exceeded (wait and resume)

**Implementation Details:**
- [ ] Reuse `RateLimiter` class from LinkedInPoller
- [ ] Configure for 50 requests/hour (vs LinkedIn's higher limit)
- [ ] Add random delays between requests (2-5 seconds)
- [ ] Handle rate limit exceeded gracefully (wait and resume)
- [ ] Log rate limiting events

### AC 5: Database Integration
**Implementation Details:**
- [ ] Insert new jobs into `jobs` table via JobsRepository
- [ ] Set platform_source='seek'
- [ ] Check for duplicates using job_url (basic duplicate detection)
- [ ] Skip duplicate jobs with logging
- [ ] Create application_tracking records with status='discovered'
- [ ] Use ApplicationRepository for tracking records
- [ ] Handle database constraint violations gracefully

### AC 6: Error Handling (REQ-003)
**Copied from PRD:**
7. Error handling:
   - Invalid HTML structure (log and skip job)
   - Network timeouts (retry with backoff)
   - Rate limit exceeded (wait and resume)

**Implementation Details:**
- [ ] Handle invalid HTML structure (log and skip job)
- [ ] Handle network timeouts (retry with exponential backoff)
- [ ] Handle rate limit exceeded (wait and resume)
- [ ] Handle missing required fields (log and skip job with details)
- [ ] Handle parsing errors (log error details, continue polling)
- [ ] Always continue polling after errors (no fatal failures)
- [ ] Retry logic: 3 attempts with backoff (5s, 15s, 45s)

### AC 7: Poller Configuration
**Implementation Details:**
- [ ] Add seek section to config/search.yaml:
  ```yaml
  seek:
    enabled: true
    search_terms: ["data engineer", "data engineering"]
    location: "Australia"
    exclude_keywords: []  # Optional filtering
  ```
- [ ] Add seek section to config/platforms.yaml:
  ```yaml
  seek:
    enabled: true
    polling_interval_minutes: 60  # Same as LinkedIn
    rate_limit_requests_per_hour: 50
    delay_between_requests_seconds: [2, 5]  # Random range
    user_agent: "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36"
    max_pages_per_search: 5  # Limit pagination depth
    retry_attempts: 3
    retry_backoff_seconds: [5, 15, 45]
  ```

### AC 8: Metrics and Logging
**Implementation Details:**
- [ ] Track poller metrics:
  - jobs_found: Total jobs discovered in search results
  - jobs_inserted: New jobs added to database
  - duplicates_skipped: Jobs already in database
  - errors: Count of errors encountered
  - pages_scraped: Number of search result pages processed
- [ ] Log each discovered job (DEBUG level)
- [ ] Log poller summary at end of each cycle (INFO level)
- [ ] Log errors with stack traces (ERROR level)
- [ ] Log rate limiting events (INFO level)
- [ ] Log retry attempts (WARNING level)

### AC 9: Testing Requirements
**Implementation Details:**
- [ ] Unit tests for SEEKPoller class
- [ ] Test metadata extraction with sample SEEK HTML
- [ ] Test salary parsing for all SEEK formats
- [ ] Test location normalization
- [ ] Test duplicate detection (job_url matching)
- [ ] Test error handling scenarios
- [ ] Test rate limiting behavior
- [ ] Mock HTTP responses for reliable testing
- [ ] Minimum 80% code coverage (excluding operational loop)
- [ ] Integration test with real database

### AC 10: Poller Scheduling Integration
**Copied from PRD:**
3. Poller runs on same interval as LinkedIn (1 hour)
4. New jobs inserted into `jobs` table with platform_source='seek'

**Implementation Details:**
- [ ] Discovered jobs flow into agent pipeline
- [ ] Jobs created with status='discovered' in application_tracking
- [ ] Ready for JobMatcher agent processing
- [ ] Poller can run standalone or as part of discovery service
- [ ] Graceful shutdown on SIGTERM/SIGINT

## Implementation Tasks Breakdown

### Task 1: Setup and Structure (30 minutes)
**Owner:** DEV Agent
**Dependencies:** None

1. Install BeautifulSoup4 if not present:
   ```bash
   pip install beautifulsoup4
   # or add to pyproject.toml dependencies
   ```

2. Create SEEKPoller class skeleton:
   ```python
   # app/pollers/seek_poller.py
   class SEEKPoller:
       def __init__(self, config, jobs_repo, app_repo):
           # Initialize with dependencies
           pass

       def search_jobs(self, keywords, location):
           # Main search method
           pass

       def extract_job_metadata(self, raw_html):
           # Parse job data from HTML
           pass

       def run_once(self):
           # Execute one poll cycle
           pass
   ```

3. Add configuration to config/search.yaml and config/platforms.yaml

4. Create basic test file structure:
   ```python
   # tests/unit/pollers/test_seek_poller.py
   # tests/integration/pollers/test_seek_integration.py
   ```

**Acceptance:**
- SEEKPoller class exists with skeleton methods
- Configuration files updated
- Test files created
- No implementation errors

### Task 2: Web Scraping Implementation (60 minutes)
**Owner:** DEV Agent
**Dependencies:** Task 1

1. Implement SEEK search URL construction:
   ```python
   def _build_search_url(self, keywords: str, location: str, page: int = 1) -> str:
       # Format: https://www.seek.com.au/data-engineer-jobs/in-All-Australia?page=1
       base_url = "https://www.seek.com.au"
       search_term = "-".join(keywords.lower().split())
       location_slug = self._normalize_location_slug(location)
       url = f"{base_url}/{search_term}-jobs/in-{location_slug}"
       if page > 1:
           url += f"?page={page}"
       return url
   ```

2. Implement HTML fetching with rate limiting:
   ```python
   def _fetch_page(self, url: str) -> str:
       self.rate_limiter.wait_if_needed()
       delay = random.uniform(self.min_delay, self.max_delay)
       time.sleep(delay)

       headers = {"User-Agent": self.config["seek"]["user_agent"]}
       response = requests.get(url, headers=headers, timeout=30)
       response.raise_for_status()
       return response.text
   ```

3. Implement pagination handling:
   ```python
   def _get_all_job_listings(self, keywords: str, location: str) -> list:
       jobs = []
       max_pages = self.config["seek"]["max_pages_per_search"]

       for page in range(1, max_pages + 1):
           url = self._build_search_url(keywords, location, page)
           html = self._fetch_page(url)
           page_jobs = self._parse_job_listings(html)

           if not page_jobs:
               break  # No more results

           jobs.extend(page_jobs)

       return jobs
   ```

4. Implement job listing extraction:
   ```python
   def _parse_job_listings(self, html: str) -> list:
       soup = BeautifulSoup(html, 'html.parser')
       job_cards = soup.find_all('article', {'data-automation': 'normalJob'})

       jobs = []
       for card in job_cards:
           job_data = {
               'title': card.find('a', {'data-automation': 'jobTitle'}).text.strip(),
               'company': card.find('a', {'data-automation': 'jobCompany'}).text.strip(),
               'location': card.find('a', {'data-automation': 'jobLocation'}).text.strip(),
               'job_url': card.find('a', {'data-automation': 'jobTitle'})['href'],
               # ... more fields
           }
           jobs.append(job_data)

       return jobs
   ```

**Acceptance:**
- URL construction works for various search terms
- HTML fetching includes rate limiting and delays
- Pagination handles multiple pages correctly
- Job listings extracted from HTML successfully
- Unit tests pass for web scraping logic

### Task 3: Metadata Extraction and Parsing (45 minutes)
**Owner:** DEV Agent
**Dependencies:** Task 2

1. Implement full job metadata extraction:
   ```python
   def extract_job_metadata(self, job_card_html: str, job_detail_html: str = None) -> Job:
       # Parse basic fields from job card
       # Optionally fetch full details from job page
       # Return Job model instance
       pass
   ```

2. Implement SEEK-specific salary parsing:
   ```python
   def _parse_seek_salary(self, salary_str: str | None) -> Decimal | None:
       """
       Parse SEEK salary formats:
       - "$100,000 - $120,000 per annum" â†’ $522/day average
       - "$1000 per day" â†’ $1000
       - "$1000-$1200 per day" â†’ $1100
       - "Competitive salary" â†’ None
       """
       if not salary_str or "competitive" in salary_str.lower():
           return None

       # Extract numbers
       numbers = re.findall(r'[\d,]+', salary_str)
       numbers = [Decimal(n.replace(',', '')) for n in numbers]

       # Determine if annual or daily
       if "per annum" in salary_str.lower():
           # Convert to daily (230 working days)
           avg_annual = sum(numbers) / len(numbers)
           return avg_annual / 230
       elif "per day" in salary_str.lower():
           return sum(numbers) / len(numbers)
       elif "per hour" in salary_str.lower():
           # Assume 8 hour day
           avg_hourly = sum(numbers) / len(numbers)
           return avg_hourly * 8

       return None
   ```

3. Implement location normalization:
   ```python
   def _normalize_location(self, location_str: str) -> str:
       """
       Normalize SEEK location formats:
       - "Melbourne CBD" â†’ "Melbourne, VIC"
       - "Remote - VIC" â†’ "Remote - VIC"
       - "Hobart, TAS" â†’ "Hobart, TAS"
       """
       # Basic normalization logic
       return location_str.strip()
   ```

4. Implement posting date extraction:
   ```python
   def _parse_posting_date(self, date_str: str) -> date:
       """
       Parse SEEK posting date:
       - "2d ago" â†’ 2 days ago
       - "1w ago" â†’ 1 week ago
       - "15/01/2025" â†’ date object
       """
       # Date parsing logic
       pass
   ```

**Acceptance:**
- All job fields extracted correctly
- Salary parsing handles all SEEK formats
- Location normalization works for common formats
- Posting date conversion accurate
- Unit tests pass for all parsing functions

### Task 4: Database Integration (30 minutes)
**Owner:** DEV Agent
**Dependencies:** Task 3

1. Integrate with JobsRepository:
   ```python
   def _store_job(self, job: Job) -> str | None:
       # Check for duplicate
       existing = self.jobs_repo.get_job_by_url(job.job_url)
       if existing:
           logger.debug(f"Duplicate job skipped: {job.job_url}")
           self.metrics["duplicates_skipped"] += 1
           return None

       # Insert job
       job_id = self.jobs_repo.insert_job(job)
       self.metrics["jobs_inserted"] += 1
       logger.info(f"New job inserted: {job.job_title} at {job.company_name}")

       return job_id
   ```

2. Create application tracking records:
   ```python
   def _create_application_tracking(self, job_id: str) -> str:
       application = Application(
           job_id=job_id,
           status="discovered",
           current_stage=None,
           completed_stages=[],
           stage_outputs={},
           error_info=None
       )

       app_id = self.app_repo.insert_application(application)
       return app_id
   ```

3. Implement full poll cycle:
   ```python
   def run_once(self) -> dict:
       logger.info("SEEK poller started")
       start_time = time.time()

       # Load search criteria
       keywords = self.config["search"]["seek"]["search_terms"]
       location = self.config["search"]["seek"]["location"]

       # Search jobs
       jobs = self._get_all_job_listings(keywords, location)
       self.metrics["jobs_found"] = len(jobs)

       # Process each job
       for job_data in jobs:
           try:
               job = self.extract_job_metadata(job_data)
               job_id = self._store_job(job)

               if job_id:
                   self._create_application_tracking(job_id)
           except Exception as e:
               logger.error(f"Error processing job: {e}")
               self.metrics["errors"] += 1

       # Log summary
       duration = time.time() - start_time
       logger.info(f"SEEK poll complete: {self.metrics} (duration: {duration:.1f}s)")

       return self.metrics
   ```

**Acceptance:**
- Jobs inserted into database successfully
- Application tracking records created
- Duplicate detection prevents re-insertion
- Database errors handled gracefully
- Integration tests pass

### Task 5: Error Handling and Testing (45 minutes)
**Owner:** QA Agent
**Dependencies:** Task 4

1. Comprehensive error handling:
   ```python
   @retry_with_backoff(max_attempts=3, backoff_seconds=[5, 15, 45])
   def _fetch_page_with_retry(self, url: str) -> str:
       # Fetch with automatic retry on connection errors
       return self._fetch_page(url)

   def _handle_parsing_error(self, job_data: dict, error: Exception) -> None:
       logger.warning(f"Failed to parse job {job_data.get('job_url', 'unknown')}: {error}")
       self.metrics["errors"] += 1
       # Continue with other jobs
   ```

2. Unit tests (minimum 80% coverage):
   ```python
   # tests/unit/pollers/test_seek_poller.py

   def test_seek_url_construction():
       # Test URL building for various inputs
       pass

   def test_salary_parsing_annual_range():
       # Test: "$100,000 - $120,000 per annum" â†’ $478.26/day average
       pass

   def test_salary_parsing_daily_rate():
       # Test: "$1000 per day" â†’ $1000
       pass

   def test_duplicate_detection():
       # Test skipping duplicate job_url
       pass

   def test_error_handling_invalid_html():
       # Test graceful handling of malformed HTML
       pass
   ```

3. Integration tests:
   ```python
   # tests/integration/pollers/test_seek_integration.py

   def test_end_to_end_seek_polling(test_db):
       # Test full flow: scrape â†’ parse â†’ store
       pass

   def test_duplicate_handling_with_db(test_db):
       # Test duplicate detection with real database
       pass
   ```

4. Manual testing with saved HTML:
   - Save sample SEEK search result pages
   - Test parsing with various job formats
   - Verify all edge cases handled

**Acceptance:**
- All error scenarios handled gracefully
- Unit tests achieve 80%+ coverage
- Integration tests pass
- Manual testing successful
- No regressions in existing tests

## Test Strategy

### Unit Tests
**Location:** `tests/unit/pollers/test_seek_poller.py`

**Test Cases:**
1. **URL Construction**
   - Test search term formatting
   - Test location slug generation
   - Test pagination parameters

2. **Salary Parsing**
   - Annual salary ranges
   - Daily rates
   - Hourly rates
   - "Competitive" (returns None)
   - Missing salary data

3. **Location Normalization**
   - Melbourne CBD
   - Remote variations
   - State abbreviations
   - Hybrid formats

4. **Date Parsing**
   - "2d ago" format
   - "1w ago" format
   - Absolute dates
   - Edge cases

5. **Duplicate Detection**
   - Matching job_url
   - Case sensitivity
   - URL variations

6. **Error Handling**
   - Network timeouts
   - Invalid HTML
   - Missing required fields
   - Rate limit scenarios

7. **Rate Limiting**
   - Wait behavior at limit
   - Call tracking accuracy
   - Time window management

**Mocking Strategy:**
- Mock `requests.get()` for HTTP responses
- Mock JobsRepository and ApplicationRepository
- Use sample SEEK HTML files for parsing tests
- Mock time.sleep() for rate limiter tests

**Coverage Target:** 80% minimum (excluding operational loop)

### Integration Tests
**Location:** `tests/integration/pollers/test_seek_integration.py`

**Test Cases:**
1. **End-to-End Flow**
   - Mock HTTP, real database
   - Verify jobs inserted correctly
   - Verify application tracking created

2. **Duplicate Handling**
   - Insert same job twice
   - Verify second insertion skipped
   - Verify metrics updated

3. **Batch Processing**
   - Process multiple jobs
   - Verify all inserted correctly
   - Verify partial failures handled

4. **Configuration Loading**
   - Load from YAML files
   - Verify settings applied correctly

**Database:** Use test DuckDB instance, cleanup after tests

### Manual Testing Checklist
- [ ] Run poller with --dry-run flag
- [ ] Verify jobs appear in database
- [ ] Check application_tracking records
- [ ] Run twice to test duplicate detection
- [ ] Monitor logs for errors
- [ ] Test with real SEEK website (if possible)
- [ ] Verify rate limiting works in practice

## Technical Notes

### SEEK Website Structure
**Search URL Format:**
```
https://www.seek.com.au/data-engineer-jobs/in-All-Australia?page=1
```

**Job Card HTML Structure (Approximate):**
```html
<article data-automation="normalJob">
  <a data-automation="jobTitle" href="/job/12345">Senior Data Engineer</a>
  <a data-automation="jobCompany">Tech Corp</a>
  <span data-automation="jobLocation">Melbourne VIC</span>
  <span data-automation="jobSalary">$100,000 - $120,000 per annum</span>
  <time datetime="2025-01-15">2d ago</time>
</article>
```

**Note:** Actual HTML structure may vary. Inspect live pages for accurate selectors.

### Salary Conversion Logic
```python
# Annual to daily conversion
annual_salary = 110000  # $110k
daily_rate = annual_salary / 230  # 230 working days/year
# Result: $478.26/day

# Hourly to daily conversion
hourly_rate = 60  # $60/hour
daily_rate = hourly_rate * 8  # 8 hour day
# Result: $480/day
```

### Anti-Bot Considerations
1. **User Agent Rotation:** Use realistic user agent strings
2. **Request Delays:** Random delays (2-5s) between requests
3. **Rate Limiting:** Max 50 requests/hour
4. **robots.txt:** Respect SEEK's robots.txt rules
5. **Fallback Plan:** Use Playwright if BeautifulSoup blocked

### Reusable Components from LinkedInPoller
- `RateLimiter` class (identical implementation)
- Retry decorator pattern
- Metrics tracking structure
- Configuration loading approach
- Database integration patterns

### Browser MCP Fallback (If Needed)
If SEEK blocks BeautifulSoup (JavaScript rendering required):

```python
# Use Docker MCP Gateway browser tools
def _fetch_page_with_playwright(self, url: str) -> str:
    browser_mcp = self.mcp_client.get_browser()
    browser_mcp.navigate(url)
    browser_mcp.wait_for("job listings loaded")
    html = browser_mcp.snapshot()  # Get rendered HTML
    return html
```

## Definition of Done

### Code Complete
- [ ] SEEKPoller class implemented in app/pollers/seek_poller.py
- [ ] Web scraping working (BeautifulSoup or Playwright)
- [ ] Metadata extraction for all required fields
- [ ] Salary parsing handles all SEEK formats
- [ ] Location normalization working
- [ ] Rate limiting enforced (50 req/hour)
- [ ] Database integration complete (jobs + application_tracking)
- [ ] Error handling comprehensive
- [ ] Configuration added to search.yaml and platforms.yaml

### Testing Complete
- [ ] Unit tests written and passing (80%+ coverage)
- [ ] Integration tests written and passing
- [ ] Manual testing completed successfully
- [ ] No regressions (all existing tests still pass)
- [ ] Edge cases tested (missing data, errors, etc.)

### Quality Gates
- [ ] Code reviewed and approved
- [ ] Architecture review passed
- [ ] Security review passed (no credential exposure)
- [ ] Performance acceptable (poll cycle <10 seconds for 50 jobs)
- [ ] Documentation complete (docstrings, comments)

### Deployment Ready
- [ ] Configuration templates updated
- [ ] Dependencies added to pyproject.toml
- [ ] Migration script (if needed) created
- [ ] Runbook updated with SEEK poller instructions

## Complexity Assessment

**Overall Complexity:** MEDIUM

### Complexity Factors

**Medium Complexity (+):**
- Web scraping (HTML parsing, variability)
- SEEK-specific format handling (salary, location)
- Anti-bot measures (user agent, delays)
- Pagination logic
- Multiple error scenarios to handle

**Low Complexity (-):**
- Reuse LinkedInPoller patterns
- RateLimiter already implemented
- Repository pattern established
- Database schema already supports platform_source='seek'
- Configuration system already in place

### Effort Estimate
**Total: 3.5 hours**
- Setup and Structure: 30 min
- Web Scraping: 60 min
- Metadata Extraction: 45 min
- Database Integration: 30 min
- Error Handling & Testing: 45 min

### Risk Factors
1. **SEEK Anti-Bot Measures:** May require Playwright fallback (low risk)
2. **HTML Structure Changes:** SEEK may update their HTML (medium risk)
3. **Rate Limiting:** Need to respect limits to avoid IP blocks (low risk)

**Mitigation:**
- Save sample HTML files for testing (decoupled from live site)
- Implement Playwright fallback from start
- Conservative rate limiting (50/hour with delays)

## Architectural Changes Needed

**None Required** - This story follows the existing architecture:
- Uses established Repository pattern
- Follows LinkedInPoller structure
- No database schema changes needed
- Configuration system already supports multiple platforms
- Agent pipeline ready to process SEEK jobs

**Minor Additions:**
- New poller class (app/pollers/seek_poller.py)
- Configuration sections in existing YAML files
- No changes to data models or database schema

## Story Retrospective Template

### What Went Well âœ…
- (To be filled after completion)

### What Could Be Improved ðŸ”§
- (To be filled after completion)

### Blockers Encountered ðŸš§
- (To be filled after completion)

### Technical Debt Created ðŸ“
- (To be filled after completion)

### Lessons Learned ðŸ’¡
- (To be filled after completion)

### Metrics ðŸ“Š
- Test Pass Rate: __%
- Code Coverage: __%
- Actual Duration: __ hours
- Story Points: Medium (3.5 hours estimated)

---

**Story Created By:** SM Agent (Scrum Master)
**Story Status:** Ready for Implementation
**Next Steps:** Assign to DEV Agent for implementation
